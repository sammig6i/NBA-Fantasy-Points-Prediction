[2024-09-19T00:07:38.307+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T00:07:38.330+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T00:07:38.348+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T00:07:38.349+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2024-09-19T00:07:38.370+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T00:07:38.379+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp4tnkx5k_']
[2024-09-19T00:07:38.383+0000] {standard_task_runner.py:105} INFO - Job 3: Subtask run_data_ingestion
[2024-09-19T00:07:38.385+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=205) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T00:07:38.386+0000] {standard_task_runner.py:72} INFO - Started process 208 to run task
[2024-09-19T00:07:38.443+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host b13ba1c84b85
[2024-09-19T00:07:38.559+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T00:07:38.560+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T00:07:38.595+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T00:07:38.606+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T00:07:38.923+0000] {docker.py:438} INFO - Traceback (most recent call last):
[2024-09-19T00:07:38.923+0000] {docker.py:438} INFO -   File "/app/main.py", line 9, in <module>
[2024-09-19T00:07:38.924+0000] {docker.py:438} INFO -     from data_pipeline_services.data_ingestion.scraper import extract_player_data, get_box_score_links, get_month_links
[2024-09-19T00:07:38.924+0000] {docker.py:438} INFO - ModuleNotFoundError: No module named 'data_pipeline_services'
[2024-09-19T00:07:39.137+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http+docker://localhost/v1.47/containers/create

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 372, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http+docker://localhost/v1.47/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /host_mnt/private/tmp/airflowtmpgjbqd1gu")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 381, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 446, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-09-19T00:07:39.152+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T000738, end_date=20240919T000739
[2024-09-19T00:07:39.171+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T00:07:39.172+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 3 for task run_data_ingestion (Docker container failed: {'StatusCode': 1}; 208)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http+docker://localhost/v1.47/containers/create

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 372, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http+docker://localhost/v1.47/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /host_mnt/private/tmp/airflowtmpgjbqd1gu")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 381, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 446, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-09-19T00:07:39.225+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T00:07:39.280+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T00:07:39.282+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T00:41:58.098+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T00:41:58.119+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T00:41:58.131+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T00:41:58.131+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2024-09-19T00:41:58.149+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T00:41:58.158+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp1mtj2hn2']
[2024-09-19T00:41:58.163+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T00:41:58.164+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=210) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T00:41:58.166+0000] {standard_task_runner.py:72} INFO - Started process 212 to run task
[2024-09-19T00:41:58.242+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 60f4d263d5b1
[2024-09-19T00:41:58.371+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T00:41:58.373+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T00:41:58.410+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T00:41:58.422+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T00:42:31.462+0000] {docker.py:438} INFO - Processing batch 1/1
[2024-09-19T00:42:31.463+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202406060BOS.html for game date 2024-06-06
[2024-09-19T00:42:31.463+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202406090BOS.html for game date 2024-06-09
[2024-09-19T00:42:31.464+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202406120DAL.html for game date 2024-06-12
[2024-09-19T00:42:31.464+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202406140DAL.html for game date 2024-06-14
[2024-09-19T00:42:31.465+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202406170BOS.html for game date 2024-06-17
[2024-09-19T00:42:31.465+0000] {docker.py:438} INFO - 2024-09-19 00:42:31,460 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7fa302eb5720>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T00:42:31.868+0000] {docker.py:438} INFO - 2024-09-19 00:42:31,865 - WARNING - Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7fa302eb5660>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T00:42:32.676+0000] {docker.py:438} INFO - 2024-09-19 00:42:32,673 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7fa301dc4310>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T00:42:34.283+0000] {docker.py:438} INFO - 2024-09-19 00:42:34,281 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7fa301dc4160>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T00:42:37.492+0000] {docker.py:438} INFO - 2024-09-19 00:42:37,490 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7fa301dc4460>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T00:42:37.497+0000] {docker.py:438} INFO - Failed to upload player_box_scores/2023-24/nba_player_stats_2023-24_2024-06-01_to_2024-06-30_2024-09-19_00-42-31.csv: HTTPConnectionPool(host='minio', port=9000): Max retries exceeded with url: /nba-raw-data?location= (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7fa301dc4610>: Failed to resolve 'minio' ([Errno -2] Name or service not known)"))
[2024-09-19T00:42:37.497+0000] {docker.py:438} INFO - 2024-09-19 00:42:37,496 - INFO - Data successfully uploaded to MinIO bucket 'nba-raw-data' as 'player_box_scores/2023-24/nba_player_stats_2023-24_2024-06-01_to_2024-06-30_2024-09-19_00-42-31.csv'
[2024-09-19T00:42:37.498+0000] {docker.py:438} INFO - 2024-09-19 00:42:37,496 - INFO - Data ingestion completed successfully
[2024-09-19T00:42:37.827+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T00:42:37.828+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T004158, end_date=20240919T004237
[2024-09-19T00:42:37.861+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T00:42:37.885+0000] {taskinstance.py:3900} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-19T00:42:37.886+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T00:58:49.929+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T00:58:49.952+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T00:58:49.963+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T00:58:49.964+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2024-09-19T00:58:49.982+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T00:58:49.991+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpnjizrxrb']
[2024-09-19T00:58:49.996+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=201) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T00:58:49.997+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T00:58:49.997+0000] {standard_task_runner.py:72} INFO - Started process 202 to run task
[2024-09-19T00:58:50.056+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 2c1409e97385
[2024-09-19T00:58:50.165+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T00:58:50.166+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T00:58:50.196+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T00:58:50.205+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T01:00:12.466+0000] {docker.py:438} INFO - Processing batch 1/1
[2024-09-19T01:00:12.468+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240DEN.html for game date 2023-10-24
[2024-09-19T01:00:12.469+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240GSW.html for game date 2023-10-24
[2024-09-19T01:00:12.469+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250ORL.html for game date 2023-10-25
[2024-09-19T01:00:12.470+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250NYK.html for game date 2023-10-25
[2024-09-19T01:00:12.470+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250IND.html for game date 2023-10-25
[2024-09-19T01:00:12.471+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHO.html for game date 2023-10-25
[2024-09-19T01:00:12.471+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MIA.html for game date 2023-10-25
[2024-09-19T01:00:12.471+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250TOR.html for game date 2023-10-25
[2024-09-19T01:00:12.472+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250BRK.html for game date 2023-10-25
[2024-09-19T01:00:12.472+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MEM.html for game date 2023-10-25
[2024-09-19T01:00:12.473+0000] {docker.py:438} INFO - Skipping incomplete data for ja morant
[2024-09-19T01:00:12.473+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHI.html for game date 2023-10-25
[2024-09-19T01:00:12.474+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250UTA.html for game date 2023-10-25
[2024-09-19T01:00:12.474+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250SAS.html for game date 2023-10-25
[2024-09-19T01:00:12.475+0000] {docker.py:438} INFO - Skipping incomplete data for devonte' graham
[2024-09-19T01:00:12.475+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250LAC.html for game date 2023-10-25
[2024-09-19T01:00:12.475+0000] {docker.py:438} INFO - File player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_01-00-12.csv successfully uploaded to bucket nba-raw-data
[2024-09-19T01:00:12.476+0000] {docker.py:438} INFO - 2024-09-19 01:00:12,465 - INFO - Data successfully uploaded to MinIO bucket 'nba-raw-data' as 'player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_01-00-12.csv'
[2024-09-19T01:00:12.476+0000] {docker.py:438} INFO - 2024-09-19 01:00:12,465 - INFO - Data ingestion completed successfully
[2024-09-19T01:00:12.707+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T01:00:12.708+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T005849, end_date=20240919T010012
[2024-09-19T01:00:12.728+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T01:00:12.752+0000] {taskinstance.py:3900} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-19T01:00:12.754+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T01:20:44.332+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T01:20:44.352+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T01:20:44.365+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T01:20:44.366+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2024-09-19T01:20:44.383+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T01:20:44.390+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp07jza_7a']
[2024-09-19T01:20:44.395+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T01:20:44.396+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=195) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T01:20:44.397+0000] {standard_task_runner.py:72} INFO - Started process 197 to run task
[2024-09-19T01:20:44.452+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 87bbd8b92491
[2024-09-19T01:20:44.567+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T01:20:44.568+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T01:20:44.605+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T01:20:44.615+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T01:22:01.930+0000] {docker.py:438} INFO - Processing batch 1/1
[2024-09-19T01:22:01.930+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240DEN.html for game date 2023-10-24
[2024-09-19T01:22:01.931+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240GSW.html for game date 2023-10-24
[2024-09-19T01:22:01.931+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250ORL.html for game date 2023-10-25
[2024-09-19T01:22:01.932+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250NYK.html for game date 2023-10-25
[2024-09-19T01:22:01.932+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250IND.html for game date 2023-10-25
[2024-09-19T01:22:01.933+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHO.html for game date 2023-10-25
[2024-09-19T01:22:01.933+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MIA.html for game date 2023-10-25
[2024-09-19T01:22:01.934+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250TOR.html for game date 2023-10-25
[2024-09-19T01:22:01.934+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250BRK.html for game date 2023-10-25
[2024-09-19T01:22:01.934+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MEM.html for game date 2023-10-25
[2024-09-19T01:22:01.935+0000] {docker.py:438} INFO - Skipping incomplete data for ja morant
[2024-09-19T01:22:01.935+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHI.html for game date 2023-10-25
[2024-09-19T01:22:01.936+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250UTA.html for game date 2023-10-25
[2024-09-19T01:22:01.936+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250SAS.html for game date 2023-10-25
[2024-09-19T01:22:01.936+0000] {docker.py:438} INFO - Skipping incomplete data for devonte' graham
[2024-09-19T01:22:01.937+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250LAC.html for game date 2023-10-25
[2024-09-19T01:22:01.937+0000] {docker.py:438} INFO - File player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_01-22-01.csv successfully uploaded to bucket nba-raw-data
[2024-09-19T01:22:01.938+0000] {docker.py:438} INFO - 2024-09-19 01:22:01,927 - INFO - Data successfully uploaded to MinIO bucket 'nba-raw-data' as 'player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_01-22-01.csv'
[2024-09-19T01:22:01.938+0000] {docker.py:438} INFO - 2024-09-19 01:22:01,927 - INFO - Data ingestion completed successfully
[2024-09-19T01:22:02.204+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T01:22:02.204+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T012044, end_date=20240919T012202
[2024-09-19T01:22:02.246+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T01:22:02.269+0000] {taskinstance.py:3900} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-19T01:22:02.271+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T03:33:43.021+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T03:33:43.040+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T03:33:43.050+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T03:33:43.050+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2024-09-19T03:33:43.064+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T03:33:43.071+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp6gu7h0c8']
[2024-09-19T03:33:43.074+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T03:33:43.076+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=205) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T03:33:43.078+0000] {standard_task_runner.py:72} INFO - Started process 206 to run task
[2024-09-19T03:33:43.127+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 96c74b18ef51
[2024-09-19T03:33:43.223+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T03:33:43.225+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T03:33:43.236+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 27, in run_data_ingestion
    return DockerOperator(
           ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 473, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'image'
[2024-09-19T03:33:43.245+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T033343, end_date=20240919T033343
[2024-09-19T03:33:43.262+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T03:33:43.262+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion (missing keyword argument 'image'; 206)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 27, in run_data_ingestion
    return DockerOperator(
           ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 473, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'image'
[2024-09-19T03:33:43.306+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T03:33:43.330+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T03:33:43.332+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T04:56:08.502+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T04:56:08.519+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T04:56:08.528+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T04:56:08.528+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2024-09-19T04:56:08.540+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T04:56:08.546+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp_9t05r5y']
[2024-09-19T04:56:08.549+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T04:56:08.550+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=210) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T04:56:08.552+0000] {standard_task_runner.py:72} INFO - Started process 211 to run task
[2024-09-19T04:56:08.596+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host e600337142de
[2024-09-19T04:56:08.690+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T04:56:08.691+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T04:56:08.696+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T04:56:08.728+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T04:56:08.737+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T04:57:21.618+0000] {docker.py:438} INFO - Processing batch 1/1
[2024-09-19T04:57:21.619+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240DEN.html for game date 2023-10-24
[2024-09-19T04:57:21.620+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240GSW.html for game date 2023-10-24
[2024-09-19T04:57:21.620+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250ORL.html for game date 2023-10-25
[2024-09-19T04:57:21.621+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250NYK.html for game date 2023-10-25
[2024-09-19T04:57:21.621+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250IND.html for game date 2023-10-25
[2024-09-19T04:57:21.622+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHO.html for game date 2023-10-25
[2024-09-19T04:57:21.622+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MIA.html for game date 2023-10-25
[2024-09-19T04:57:21.623+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250TOR.html for game date 2023-10-25
[2024-09-19T04:57:21.623+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250BRK.html for game date 2023-10-25
[2024-09-19T04:57:21.624+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MEM.html for game date 2023-10-25
[2024-09-19T04:57:21.624+0000] {docker.py:438} INFO - Skipping incomplete data for ja morant
[2024-09-19T04:57:21.625+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHI.html for game date 2023-10-25
[2024-09-19T04:57:21.625+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250UTA.html for game date 2023-10-25
[2024-09-19T04:57:21.626+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250SAS.html for game date 2023-10-25
[2024-09-19T04:57:21.626+0000] {docker.py:438} INFO - Skipping incomplete data for devonte' graham
[2024-09-19T04:57:21.627+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250LAC.html for game date 2023-10-25
[2024-09-19T04:57:21.627+0000] {docker.py:438} INFO - Failed to upload player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_04-57-21.csv: expected string or bytes-like object
[2024-09-19T04:57:21.627+0000] {docker.py:438} INFO - 2024-09-19 04:57:21,617 - INFO - Data successfully uploaded to MinIO bucket 'None' as 'player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_04-57-21.csv'
[2024-09-19T04:57:21.628+0000] {docker.py:438} INFO - 2024-09-19 04:57:21,617 - INFO - Data ingestion completed successfully
[2024-09-19T04:57:21.884+0000] {python.py:240} INFO - Done. Returned value was: Processing batch 1/1
Scraping box score: https://www.basketball-reference.com/boxscores/202310240DEN.html for game date 2023-10-24
Scraping box score: https://www.basketball-reference.com/boxscores/202310240GSW.html for game date 2023-10-24
Scraping box score: https://www.basketball-reference.com/boxscores/202310250ORL.html for game date 2023-10-25
Scraping box score: https://www.basketball-reference.com/boxscores/202310250NYK.html for game date 2023-10-25
Scraping box score: https://www.basketball-reference.com/boxscores/202310250IND.html for game date 2023-10-25
Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHO.html for game date 2023-10-25
Scraping box score: https://www.basketball-reference.com/boxscores/202310250MIA.html for game date 2023-10-25
Scraping box score: https://www.basketball-reference.com/boxscores/202310250TOR.html for game date 2023-10-25
Scraping box score: https://www.basketball-reference.com/boxscores/202310250BRK.html for game date 2023-10-25
Scraping box score: https://www.basketball-reference.com/boxscores/202310250MEM.html for game date 2023-10-25
Skipping incomplete data for ja morant
Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHI.html for game date 2023-10-25
Scraping box score: https://www.basketball-reference.com/boxscores/202310250UTA.html for game date 2023-10-25
Scraping box score: https://www.basketball-reference.com/boxscores/202310250SAS.html for game date 2023-10-25
Skipping incomplete data for devonte' graham
Scraping box score: https://www.basketball-reference.com/boxscores/202310250LAC.html for game date 2023-10-25
Failed to upload player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_04-57-21.csv: expected string or bytes-like object
2024-09-19 04:57:21,617 - INFO - Data successfully uploaded to MinIO bucket 'None' as 'player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_04-57-21.csv'
2024-09-19 04:57:21,617 - INFO - Data ingestion completed successfully
[2024-09-19T04:57:21.910+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T04:57:21.910+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T045608, end_date=20240919T045721
[2024-09-19T04:57:21.955+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T04:57:21.977+0000] {taskinstance.py:3900} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-19T04:57:21.978+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T05:31:48.310+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T05:31:48.334+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T05:31:48.344+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T05:31:48.345+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2024-09-19T05:31:48.361+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T05:31:48.370+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmph21_bhin']
[2024-09-19T05:31:48.373+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T05:31:48.375+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=200) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T05:31:48.377+0000] {standard_task_runner.py:72} INFO - Started process 202 to run task
[2024-09-19T05:31:48.432+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host c3e696a8f912
[2024-09-19T05:31:48.557+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T05:31:48.558+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T05:31:48.582+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T05:31:48.637+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T05:31:48.666+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T05:31:48.716+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http+docker://localhost/v1.47/containers/create?name=data_ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 372, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /host_mnt/private/tmp/airflowtmpd2d2csea")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: http+docker://localhost/v1.47/containers/create?name=data_ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 34, in run_data_ingestion
    ).execute(context=None)
      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 381, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 409 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Conflict ("Conflict. The container name "/data_ingestion" is already in use by container "33d0a3ab9cf0fb197887026189d4bc3b5816abcd50aa7cb3a15e026f55811102". You have to remove (or rename) that container to be able to reuse that name.")
[2024-09-19T05:31:48.736+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T053148, end_date=20240919T053148
[2024-09-19T05:31:48.762+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T05:31:48.763+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion (409 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Conflict ("Conflict. The container name "/data_ingestion" is already in use by container "33d0a3ab9cf0fb197887026189d4bc3b5816abcd50aa7cb3a15e026f55811102". You have to remove (or rename) that container to be able to reuse that name."); 202)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http+docker://localhost/v1.47/containers/create?name=data_ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 372, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /host_mnt/private/tmp/airflowtmpd2d2csea")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: http+docker://localhost/v1.47/containers/create?name=data_ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 34, in run_data_ingestion
    ).execute(context=None)
      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 381, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 409 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Conflict ("Conflict. The container name "/data_ingestion" is already in use by container "33d0a3ab9cf0fb197887026189d4bc3b5816abcd50aa7cb3a15e026f55811102". You have to remove (or rename) that container to be able to reuse that name.")
[2024-09-19T05:31:48.805+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T05:31:48.830+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T05:31:48.832+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T06:02:15.062+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T06:02:15.081+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T06:02:15.092+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T06:02:15.092+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T06:02:15.107+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T06:02:15.114+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmplu3qq_e2']
[2024-09-19T06:02:15.117+0000] {standard_task_runner.py:105} INFO - Job 3: Subtask run_data_ingestion
[2024-09-19T06:02:15.120+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=205) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T06:02:15.121+0000] {standard_task_runner.py:72} INFO - Started process 207 to run task
[2024-09-19T06:02:15.172+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 7745cb45e4df
[2024-09-19T06:02:15.272+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T06:02:15.273+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T06:02:15.277+0000] {python.py:240} INFO - Done. Returned value was: <Task(DockerOperator): run_data_ingestion>
[2024-09-19T06:02:15.285+0000] {xcom.py:690} ERROR - Object of type DockerOperator is not JSON serializable. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config or make sure to decorate your object with attr.
[2024-09-19T06:02:15.287+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 92, in default
    return serialize(o)
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 190, in serialize
    raise TypeError(f"cannot serialize object of type {cls}")
TypeError: cannot serialize object of type <class 'airflow.providers.docker.operators.docker.DockerOperator'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 789, in _execute_task
    task_instance.xcom_push(key=XCOM_RETURN_KEY, value=xcom_value, session=session_or_null)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3638, in xcom_push
    XCom.set(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 249, in set
    value = cls.serialize_value(
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 688, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 105, in encode
    return super().encode(o)
           ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 94, in default
    return super().default(o)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DockerOperator is not JSON serializable
[2024-09-19T06:02:15.301+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T060215, end_date=20240919T060215
[2024-09-19T06:02:15.316+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T06:02:15.317+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 3 for task run_data_ingestion (Object of type DockerOperator is not JSON serializable; 207)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 92, in default
    return serialize(o)
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 190, in serialize
    raise TypeError(f"cannot serialize object of type {cls}")
TypeError: cannot serialize object of type <class 'airflow.providers.docker.operators.docker.DockerOperator'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 789, in _execute_task
    task_instance.xcom_push(key=XCOM_RETURN_KEY, value=xcom_value, session=session_or_null)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3638, in xcom_push
    XCom.set(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 249, in set
    value = cls.serialize_value(
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 688, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 105, in encode
    return super().encode(o)
           ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 94, in default
    return super().default(o)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DockerOperator is not JSON serializable
[2024-09-19T06:02:15.345+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T06:02:15.371+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T06:02:15.374+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T06:09:15.518+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T06:09:15.539+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T06:09:15.551+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T06:09:15.551+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T06:09:15.566+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T06:09:15.575+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp2yixwrum']
[2024-09-19T06:09:15.578+0000] {standard_task_runner.py:105} INFO - Job 3: Subtask run_data_ingestion
[2024-09-19T06:09:15.580+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=200) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T06:09:15.581+0000] {standard_task_runner.py:72} INFO - Started process 203 to run task
[2024-09-19T06:09:15.632+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 5edf32f722bc
[2024-09-19T06:09:15.733+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T06:09:15.734+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T06:09:15.738+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T06:09:15.769+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T06:09:15.779+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: http+docker://localhost/v1.47/containers/create?name=data_ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 36, in run_data_ingestion
    ).execute(context=None)
      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 409 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Conflict ("Conflict. The container name "/data_ingestion" is already in use by container "5d2b0fbfc77ec24309e059550d37990311e783fbfe5d76fce31c179cd164ae3d". You have to remove (or rename) that container to be able to reuse that name.")
[2024-09-19T06:09:15.796+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T060915, end_date=20240919T060915
[2024-09-19T06:09:15.817+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T06:09:15.818+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 3 for task run_data_ingestion (409 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Conflict ("Conflict. The container name "/data_ingestion" is already in use by container "5d2b0fbfc77ec24309e059550d37990311e783fbfe5d76fce31c179cd164ae3d". You have to remove (or rename) that container to be able to reuse that name."); 203)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: http+docker://localhost/v1.47/containers/create?name=data_ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 36, in run_data_ingestion
    ).execute(context=None)
      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 409 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Conflict ("Conflict. The container name "/data_ingestion" is already in use by container "5d2b0fbfc77ec24309e059550d37990311e783fbfe5d76fce31c179cd164ae3d". You have to remove (or rename) that container to be able to reuse that name.")
[2024-09-19T06:09:15.848+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T06:09:15.878+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T06:09:15.880+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T06:44:40.369+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T06:44:40.392+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T06:44:40.405+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T06:44:40.406+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T06:44:40.424+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T06:44:40.432+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpce_dzat_']
[2024-09-19T06:44:40.435+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T06:44:40.438+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=357) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T06:44:40.439+0000] {standard_task_runner.py:72} INFO - Started process 359 to run task
[2024-09-19T06:44:40.497+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 2d68041b864f
[2024-09-19T06:44:40.628+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T06:44:40.630+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T06:44:40.636+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T06:44:40.667+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T06:45:38.310+0000] {local_task_job_runner.py:127} ERROR - Received SIGTERM. Terminating subprocesses
[2024-09-19T06:45:38.324+0000] {process_utils.py:132} INFO - Sending 15 to group 359. PIDs of all processes in the group: [359]
[2024-09-19T06:45:38.324+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 359
[2024-09-19T06:45:38.325+0000] {taskinstance.py:3092} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-09-19T06:45:38.333+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 434, in _run_image_with_mounts
    for log_chunk in logstream:
                     ^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/types/daemon.py", line 29, in __next__
    return next(self._stream)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 438, in <genexpr>
    gen = (data for (_, data) in gen)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/socket.py", line 112, in frames_iter_no_tty
    (stream, n) = next_frame_header(socket)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/socket.py", line 84, in next_frame_header
    data = read_exactly(socket, 8)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/socket.py", line 69, in read_exactly
    next_data = read(socket, n - len(data))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/socket.py", line 40, in read
    poll.poll()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3094, in signal_handler
    raise AirflowTaskTerminated("Task received SIGTERM signal")
airflow.exceptions.AirflowTaskTerminated: Task received SIGTERM signal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: http+docker://localhost/v1.47/containers/903b59d207ad780348cdab72d8f29f60bbbe2e0e937a5c6a17f33fa0164e0f88?v=False&link=False&force=False

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 36, in run_data_ingestion
    ).execute(context=None)
      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 464, in _run_image_with_mounts
    self.cli.remove_container(self.container["Id"])
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 1037, in remove_container
    self._raise_for_status(res)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 409 Client Error for http+docker://localhost/v1.47/containers/903b59d207ad780348cdab72d8f29f60bbbe2e0e937a5c6a17f33fa0164e0f88?v=False&link=False&force=False: Conflict ("cannot remove container "/data_ingestion": container is running: stop the container before removing or force remove")
[2024-09-19T06:45:38.356+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T064440, end_date=20240919T064538
[2024-09-19T06:45:38.374+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T06:45:38.375+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion (409 Client Error for http+docker://localhost/v1.47/containers/903b59d207ad780348cdab72d8f29f60bbbe2e0e937a5c6a17f33fa0164e0f88?v=False&link=False&force=False: Conflict ("cannot remove container "/data_ingestion": container is running: stop the container before removing or force remove"); 359)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 434, in _run_image_with_mounts
    for log_chunk in logstream:
                     ^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/types/daemon.py", line 29, in __next__
    return next(self._stream)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 438, in <genexpr>
    gen = (data for (_, data) in gen)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/socket.py", line 112, in frames_iter_no_tty
    (stream, n) = next_frame_header(socket)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/socket.py", line 84, in next_frame_header
    data = read_exactly(socket, 8)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/socket.py", line 69, in read_exactly
    next_data = read(socket, n - len(data))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/socket.py", line 40, in read
    poll.poll()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3094, in signal_handler
    raise AirflowTaskTerminated("Task received SIGTERM signal")
airflow.exceptions.AirflowTaskTerminated: Task received SIGTERM signal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: http+docker://localhost/v1.47/containers/903b59d207ad780348cdab72d8f29f60bbbe2e0e937a5c6a17f33fa0164e0f88?v=False&link=False&force=False

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 36, in run_data_ingestion
    ).execute(context=None)
      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 464, in _run_image_with_mounts
    self.cli.remove_container(self.container["Id"])
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 1037, in remove_container
    self._raise_for_status(res)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 409 Client Error for http+docker://localhost/v1.47/containers/903b59d207ad780348cdab72d8f29f60bbbe2e0e937a5c6a17f33fa0164e0f88?v=False&link=False&force=False: Conflict ("cannot remove container "/data_ingestion": container is running: stop the container before removing or force remove")
[2024-09-19T06:45:38.422+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=359, status='terminated', exitcode=1, started='06:44:39') (359) terminated with exit code 1
[2024-09-19T06:45:38.423+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 143
[2024-09-19T06:48:26.516+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T06:48:26.538+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T06:48:26.552+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T06:48:26.552+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T06:48:26.566+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T06:48:26.573+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpc9es4mge']
[2024-09-19T06:48:26.578+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T06:48:26.579+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=200) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T06:48:26.579+0000] {standard_task_runner.py:72} INFO - Started process 203 to run task
[2024-09-19T06:48:26.630+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 23cc972357c5
[2024-09-19T06:48:26.727+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T06:48:26.728+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T06:48:26.733+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T06:48:26.763+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T06:48:26.772+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: http+docker://localhost/v1.47/containers/create?name=data_ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 36, in run_data_ingestion
    ).execute(context=None)
      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 409 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Conflict ("Conflict. The container name "/data_ingestion" is already in use by container "5be40a47fb7f09fe1fbb2bff7ae6af65b1fc0bc62ebafddc556f476862bcddf3". You have to remove (or rename) that container to be able to reuse that name.")
[2024-09-19T06:48:26.802+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T064826, end_date=20240919T064826
[2024-09-19T06:48:26.865+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T06:48:26.866+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion (409 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Conflict ("Conflict. The container name "/data_ingestion" is already in use by container "5be40a47fb7f09fe1fbb2bff7ae6af65b1fc0bc62ebafddc556f476862bcddf3". You have to remove (or rename) that container to be able to reuse that name."); 203)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: http+docker://localhost/v1.47/containers/create?name=data_ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 36, in run_data_ingestion
    ).execute(context=None)
      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 409 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Conflict ("Conflict. The container name "/data_ingestion" is already in use by container "5be40a47fb7f09fe1fbb2bff7ae6af65b1fc0bc62ebafddc556f476862bcddf3". You have to remove (or rename) that container to be able to reuse that name.")
[2024-09-19T06:48:26.885+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T06:48:26.919+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T06:48:26.921+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T07:06:48.369+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T07:06:48.392+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T07:06:48.403+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T07:06:48.404+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T07:06:48.419+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T07:06:48.426+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpbmofss54']
[2024-09-19T07:06:48.429+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T07:06:48.431+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=200) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T07:06:48.432+0000] {standard_task_runner.py:72} INFO - Started process 201 to run task
[2024-09-19T07:06:48.479+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 782e24e82a2e
[2024-09-19T07:06:48.572+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T07:06:48.573+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T07:06:48.578+0000] {python.py:240} INFO - Done. Returned value was: <Task(DockerOperator): run_data_ingestion>
[2024-09-19T07:06:48.585+0000] {xcom.py:690} ERROR - Object of type DockerOperator is not JSON serializable. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config or make sure to decorate your object with attr.
[2024-09-19T07:06:48.587+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 92, in default
    return serialize(o)
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 190, in serialize
    raise TypeError(f"cannot serialize object of type {cls}")
TypeError: cannot serialize object of type <class 'airflow.providers.docker.operators.docker.DockerOperator'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 789, in _execute_task
    task_instance.xcom_push(key=XCOM_RETURN_KEY, value=xcom_value, session=session_or_null)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3638, in xcom_push
    XCom.set(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 249, in set
    value = cls.serialize_value(
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 688, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 105, in encode
    return super().encode(o)
           ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 94, in default
    return super().default(o)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DockerOperator is not JSON serializable
[2024-09-19T07:06:48.601+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T070648, end_date=20240919T070648
[2024-09-19T07:06:48.616+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T07:06:48.617+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion (Object of type DockerOperator is not JSON serializable; 201)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 92, in default
    return serialize(o)
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 190, in serialize
    raise TypeError(f"cannot serialize object of type {cls}")
TypeError: cannot serialize object of type <class 'airflow.providers.docker.operators.docker.DockerOperator'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 789, in _execute_task
    task_instance.xcom_push(key=XCOM_RETURN_KEY, value=xcom_value, session=session_or_null)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3638, in xcom_push
    XCom.set(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 249, in set
    value = cls.serialize_value(
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 688, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 105, in encode
    return super().encode(o)
           ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 94, in default
    return super().default(o)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DockerOperator is not JSON serializable
[2024-09-19T07:06:48.656+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T07:06:48.681+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T07:06:48.683+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T07:11:41.051+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T07:11:41.076+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T07:11:41.086+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T07:11:41.087+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T07:11:41.103+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T07:11:41.112+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp7u_08kz_']
[2024-09-19T07:11:41.115+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T07:11:41.118+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=205) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T07:11:41.120+0000] {standard_task_runner.py:72} INFO - Started process 208 to run task
[2024-09-19T07:11:41.173+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 8a711b9e5d7e
[2024-09-19T07:11:41.281+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T07:11:41.284+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T07:11:41.288+0000] {python.py:240} INFO - Done. Returned value was: <Task(DockerOperator): run_data_ingestion>
[2024-09-19T07:11:41.296+0000] {xcom.py:690} ERROR - Object of type DockerOperator is not JSON serializable. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config or make sure to decorate your object with attr.
[2024-09-19T07:11:41.298+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 92, in default
    return serialize(o)
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 190, in serialize
    raise TypeError(f"cannot serialize object of type {cls}")
TypeError: cannot serialize object of type <class 'airflow.providers.docker.operators.docker.DockerOperator'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 789, in _execute_task
    task_instance.xcom_push(key=XCOM_RETURN_KEY, value=xcom_value, session=session_or_null)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3638, in xcom_push
    XCom.set(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 249, in set
    value = cls.serialize_value(
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 688, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 105, in encode
    return super().encode(o)
           ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 94, in default
    return super().default(o)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DockerOperator is not JSON serializable
[2024-09-19T07:11:41.314+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T071141, end_date=20240919T071141
[2024-09-19T07:11:41.332+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T07:11:41.334+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion (Object of type DockerOperator is not JSON serializable; 208)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 92, in default
    return serialize(o)
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 190, in serialize
    raise TypeError(f"cannot serialize object of type {cls}")
TypeError: cannot serialize object of type <class 'airflow.providers.docker.operators.docker.DockerOperator'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 789, in _execute_task
    task_instance.xcom_push(key=XCOM_RETURN_KEY, value=xcom_value, session=session_or_null)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3638, in xcom_push
    XCom.set(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 249, in set
    value = cls.serialize_value(
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 688, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 105, in encode
    return super().encode(o)
           ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 94, in default
    return super().default(o)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DockerOperator is not JSON serializable
[2024-09-19T07:11:41.385+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T07:11:41.413+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T07:11:41.415+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T07:17:36.086+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T07:17:36.106+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T07:17:36.116+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T07:17:36.117+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T07:17:36.132+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T07:17:36.140+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp6on09ajr']
[2024-09-19T07:17:36.143+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T07:17:36.144+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=206) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T07:17:36.145+0000] {standard_task_runner.py:72} INFO - Started process 208 to run task
[2024-09-19T07:17:36.197+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 54773025263a
[2024-09-19T07:17:36.304+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T07:17:36.305+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T07:17:36.311+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T07:17:36.344+0000] {docker.py:496} INFO - ::group::Pulling docker image {{ var.value.DOCKER_REGISTRY }}/data-ingestion:latest
[2024-09-19T07:17:36.350+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http+docker://localhost/v1.47/images/create?tag=latest&fromImage=%7B%7B+var.value.DOCKER_REGISTRY+%7D%7D%2Fdata-ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 43, in run_data_ingestion
    ).execute(context=None)
      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 498, in execute
    for output in self.cli.pull(self.image, stream=True, decode=True):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/image.py", line 429, in pull
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 500 Server Error for http+docker://localhost/v1.47/images/create?tag=latest&fromImage=%7B%7B+var.value.DOCKER_REGISTRY+%7D%7D%2Fdata-ingestion: Internal Server Error ("parsing image {{ var.value.DOCKER_REGISTRY }}/data-ingestion: invalid reference format")
[2024-09-19T07:17:36.369+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T071736, end_date=20240919T071736
[2024-09-19T07:17:36.389+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T07:17:36.390+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion (500 Server Error for http+docker://localhost/v1.47/images/create?tag=latest&fromImage=%7B%7B+var.value.DOCKER_REGISTRY+%7D%7D%2Fdata-ingestion: Internal Server Error ("parsing image {{ var.value.DOCKER_REGISTRY }}/data-ingestion: invalid reference format"); 208)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http+docker://localhost/v1.47/images/create?tag=latest&fromImage=%7B%7B+var.value.DOCKER_REGISTRY+%7D%7D%2Fdata-ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 43, in run_data_ingestion
    ).execute(context=None)
      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 498, in execute
    for output in self.cli.pull(self.image, stream=True, decode=True):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/image.py", line 429, in pull
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 500 Server Error for http+docker://localhost/v1.47/images/create?tag=latest&fromImage=%7B%7B+var.value.DOCKER_REGISTRY+%7D%7D%2Fdata-ingestion: Internal Server Error ("parsing image {{ var.value.DOCKER_REGISTRY }}/data-ingestion: invalid reference format")
[2024-09-19T07:17:36.411+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T07:17:36.438+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T07:17:36.443+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T07:38:11.070+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T07:38:11.091+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T07:38:11.101+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T07:38:11.101+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T07:38:11.116+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T07:38:11.124+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp1str_4n8']
[2024-09-19T07:38:11.128+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T07:38:11.130+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=209) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T07:38:11.130+0000] {standard_task_runner.py:72} INFO - Started process 211 to run task
[2024-09-19T07:38:11.184+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host b014129893fa
[2024-09-19T07:38:11.296+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T07:38:11.298+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T07:38:11.319+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 30, in run_data_ingestion
    image=f"{Variable.get("DOCKER_REGISTRY")}/data-ingestion:latest",
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/variable.py", line 145, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable DOCKER_REGISTRY does not exist'
[2024-09-19T07:38:11.338+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T073811, end_date=20240919T073811
[2024-09-19T07:38:11.356+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T07:38:11.357+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion ('Variable DOCKER_REGISTRY does not exist'; 211)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 30, in run_data_ingestion
    image=f"{Variable.get("DOCKER_REGISTRY")}/data-ingestion:latest",
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/variable.py", line 145, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable DOCKER_REGISTRY does not exist'
[2024-09-19T07:38:11.399+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T07:38:11.425+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T07:38:11.427+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T07:48:13.468+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T07:48:13.488+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T07:48:13.502+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T07:48:13.504+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T07:48:13.525+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T07:48:13.534+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp0htylgva']
[2024-09-19T07:48:13.540+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=210) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T07:48:13.540+0000] {standard_task_runner.py:105} INFO - Job 3: Subtask run_data_ingestion
[2024-09-19T07:48:13.540+0000] {standard_task_runner.py:72} INFO - Started process 213 to run task
[2024-09-19T07:48:13.608+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 541b674c082f
[2024-09-19T07:48:13.718+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T07:48:13.719+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T07:48:13.724+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T07:48:13.754+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T07:49:37.313+0000] {docker.py:438} INFO - Processing batch 1/1
[2024-09-19T07:49:37.313+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240DEN.html for game date 2023-10-24
[2024-09-19T07:49:37.317+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240GSW.html for game date 2023-10-24
[2024-09-19T07:49:37.318+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250ORL.html for game date 2023-10-25
[2024-09-19T07:49:37.318+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250NYK.html for game date 2023-10-25
[2024-09-19T07:49:37.319+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250IND.html for game date 2023-10-25
[2024-09-19T07:49:37.319+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHO.html for game date 2023-10-25
[2024-09-19T07:49:37.320+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MIA.html for game date 2023-10-25
[2024-09-19T07:49:37.320+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250TOR.html for game date 2023-10-25
[2024-09-19T07:49:37.321+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250BRK.html for game date 2023-10-25
[2024-09-19T07:49:37.321+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MEM.html for game date 2023-10-25
[2024-09-19T07:49:37.322+0000] {docker.py:438} INFO - Skipping incomplete data for ja morant
[2024-09-19T07:49:37.322+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHI.html for game date 2023-10-25
[2024-09-19T07:49:37.323+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250UTA.html for game date 2023-10-25
[2024-09-19T07:49:37.324+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250SAS.html for game date 2023-10-25
[2024-09-19T07:49:37.324+0000] {docker.py:438} INFO - Skipping incomplete data for devonte' graham
[2024-09-19T07:49:37.324+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250LAC.html for game date 2023-10-25
[2024-09-19T07:49:37.325+0000] {docker.py:438} INFO - Failed to upload player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_07-49-37.csv: 'bytes' object has no attribute 'read'
[2024-09-19T07:49:37.325+0000] {docker.py:438} INFO - 2024-09-19 07:49:37,311 - ERROR - Error uploading data to MinIO: 'bytes' object has no attribute 'read'
[2024-09-19T07:49:37.634+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 57, in run_data_ingestion
    ).execute(context=kwargs)
      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 446, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-09-19T07:49:37.645+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T074813, end_date=20240919T074937
[2024-09-19T07:49:37.667+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T07:49:37.667+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 3 for task run_data_ingestion (Docker container failed: {'StatusCode': 1}; 213)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 57, in run_data_ingestion
    ).execute(context=kwargs)
      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 446, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-09-19T07:49:37.707+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T07:49:37.734+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T07:49:37.736+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T07:53:41.790+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T07:53:41.814+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T07:53:41.826+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T07:53:41.826+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T07:53:41.844+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T07:53:41.852+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpbrk5_n0o']
[2024-09-19T07:53:41.855+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T07:53:41.858+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=201) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T07:53:41.859+0000] {standard_task_runner.py:72} INFO - Started process 203 to run task
[2024-09-19T07:53:41.921+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host c4f186c0b4c0
[2024-09-19T07:53:42.024+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T07:53:42.025+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T07:53:42.030+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T07:53:42.059+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T07:53:42.068+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: http+docker://localhost/v1.47/containers/create?name=data_ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 57, in run_data_ingestion
    ).execute(context=kwargs)
      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 409 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Conflict ("Conflict. The container name "/data_ingestion" is already in use by container "b1d1bbb1b975d3426e5b8aa0ac68e3bafcd81eec820deb561a7cd967644b9dc6". You have to remove (or rename) that container to be able to reuse that name.")
[2024-09-19T07:53:42.086+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T075341, end_date=20240919T075342
[2024-09-19T07:53:42.103+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T07:53:42.105+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion (409 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Conflict ("Conflict. The container name "/data_ingestion" is already in use by container "b1d1bbb1b975d3426e5b8aa0ac68e3bafcd81eec820deb561a7cd967644b9dc6". You have to remove (or rename) that container to be able to reuse that name."); 203)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: http+docker://localhost/v1.47/containers/create?name=data_ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 57, in run_data_ingestion
    ).execute(context=kwargs)
      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 409 Client Error for http+docker://localhost/v1.47/containers/create?name=data_ingestion: Conflict ("Conflict. The container name "/data_ingestion" is already in use by container "b1d1bbb1b975d3426e5b8aa0ac68e3bafcd81eec820deb561a7cd967644b9dc6". You have to remove (or rename) that container to be able to reuse that name.")
[2024-09-19T07:53:42.123+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T07:53:42.151+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T07:53:42.153+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T08:06:20.185+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T08:06:20.207+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T08:06:20.220+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T08:06:20.221+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T08:06:20.238+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T08:06:20.248+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpojlyhqw2']
[2024-09-19T08:06:20.251+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T08:06:20.254+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=200) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T08:06:20.255+0000] {standard_task_runner.py:72} INFO - Started process 202 to run task
[2024-09-19T08:06:20.312+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 79e10942e2e4
[2024-09-19T08:06:20.427+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T08:06:20.429+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T08:06:20.434+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T08:06:20.465+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T08:07:52.773+0000] {docker.py:438} INFO - Processing batch 1/1
[2024-09-19T08:07:52.774+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240DEN.html for game date 2023-10-24
[2024-09-19T08:07:52.775+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240GSW.html for game date 2023-10-24
[2024-09-19T08:07:52.775+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250ORL.html for game date 2023-10-25
[2024-09-19T08:07:52.776+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250NYK.html for game date 2023-10-25
[2024-09-19T08:07:52.777+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250IND.html for game date 2023-10-25
[2024-09-19T08:07:52.777+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHO.html for game date 2023-10-25
[2024-09-19T08:07:52.778+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MIA.html for game date 2023-10-25
[2024-09-19T08:07:52.778+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250TOR.html for game date 2023-10-25
[2024-09-19T08:07:52.778+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250BRK.html for game date 2023-10-25
[2024-09-19T08:07:52.779+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MEM.html for game date 2023-10-25
[2024-09-19T08:07:52.779+0000] {docker.py:438} INFO - Skipping incomplete data for ja morant
[2024-09-19T08:07:52.780+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHI.html for game date 2023-10-25
[2024-09-19T08:07:52.780+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250UTA.html for game date 2023-10-25
[2024-09-19T08:07:52.781+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250SAS.html for game date 2023-10-25
[2024-09-19T08:07:52.781+0000] {docker.py:438} INFO - Skipping incomplete data for devonte' graham
[2024-09-19T08:07:52.782+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250LAC.html for game date 2023-10-25
[2024-09-19T08:07:52.782+0000] {docker.py:438} INFO - Failed to upload player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_08-07-52.csv: 'bytes' object has no attribute 'read'
[2024-09-19T08:07:52.782+0000] {docker.py:438} INFO - 2024-09-19 08:07:52,771 - ERROR - Error uploading data to MinIO: 'bytes' object has no attribute 'read'
[2024-09-19T08:07:53.072+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 57, in run_data_ingestion
    ).execute(context=kwargs)
      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 446, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-09-19T08:07:53.083+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T080620, end_date=20240919T080753
[2024-09-19T08:07:53.101+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T08:07:53.102+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion (Docker container failed: {'StatusCode': 1}; 202)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 57, in run_data_ingestion
    ).execute(context=kwargs)
      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 446, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-09-19T08:07:53.135+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T08:07:53.158+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T08:07:53.160+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T08:28:34.682+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T08:28:34.702+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T08:28:34.713+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T08:28:34.714+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T08:28:34.727+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T08:28:34.735+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp50t9gabq']
[2024-09-19T08:28:34.738+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T08:28:34.740+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=200) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T08:28:34.740+0000] {standard_task_runner.py:72} INFO - Started process 201 to run task
[2024-09-19T08:28:34.790+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host fdcd6186b62b
[2024-09-19T08:28:34.891+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T08:28:34.892+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T08:28:34.897+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T08:28:34.897+0000] {docker.py:496} INFO - ::group::Pulling docker image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T08:28:35.403+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http+docker://localhost/v1.47/images/create?tag=latest&fromImage=us-west1-docker.pkg.dev%2Fnba-fantasy-ml%2Fnba-data-pipeline-repo%2Fdata-ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 58, in run_data_ingestion
    ).execute(context=kwargs)
      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 498, in execute
    for output in self.cli.pull(self.image, stream=True, decode=True):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/image.py", line 429, in pull
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 500 Server Error for http+docker://localhost/v1.47/images/create?tag=latest&fromImage=us-west1-docker.pkg.dev%2Fnba-fantasy-ml%2Fnba-data-pipeline-repo%2Fdata-ingestion: Internal Server Error ("Head "https://us-west1-docker.pkg.dev/v2/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion/manifests/latest": denied: Unauthenticated request. Unauthenticated requests do not have permission "artifactregistry.repositories.downloadArtifacts" on resource "projects/nba-fantasy-ml/locations/us-west1/repositories/nba-data-pipeline-repo" (or it may not exist)")
[2024-09-19T08:28:35.421+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T082834, end_date=20240919T082835
[2024-09-19T08:28:35.437+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T08:28:35.438+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion (500 Server Error for http+docker://localhost/v1.47/images/create?tag=latest&fromImage=us-west1-docker.pkg.dev%2Fnba-fantasy-ml%2Fnba-data-pipeline-repo%2Fdata-ingestion: Internal Server Error ("Head "https://us-west1-docker.pkg.dev/v2/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion/manifests/latest": denied: Unauthenticated request. Unauthenticated requests do not have permission "artifactregistry.repositories.downloadArtifacts" on resource "projects/nba-fantasy-ml/locations/us-west1/repositories/nba-data-pipeline-repo" (or it may not exist)"); 201)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http+docker://localhost/v1.47/images/create?tag=latest&fromImage=us-west1-docker.pkg.dev%2Fnba-fantasy-ml%2Fnba-data-pipeline-repo%2Fdata-ingestion

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 266, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/nba_data_pipeline.py", line 58, in run_data_ingestion
    ).execute(context=kwargs)
      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 498, in execute
    for output in self.cli.pull(self.image, stream=True, decode=True):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/image.py", line 429, in pull
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 500 Server Error for http+docker://localhost/v1.47/images/create?tag=latest&fromImage=us-west1-docker.pkg.dev%2Fnba-fantasy-ml%2Fnba-data-pipeline-repo%2Fdata-ingestion: Internal Server Error ("Head "https://us-west1-docker.pkg.dev/v2/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion/manifests/latest": denied: Unauthenticated request. Unauthenticated requests do not have permission "artifactregistry.repositories.downloadArtifacts" on resource "projects/nba-fantasy-ml/locations/us-west1/repositories/nba-data-pipeline-repo" (or it may not exist)")
[2024-09-19T08:28:35.462+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T08:28:35.473+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T08:54:37.437+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T08:54:37.484+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T08:54:37.500+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T08:54:37.501+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T08:54:37.529+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T08:54:37.540+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmph1pm6cz0']
[2024-09-19T08:54:37.544+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T08:54:37.549+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=206) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T08:54:37.550+0000] {standard_task_runner.py:72} INFO - Started process 207 to run task
[2024-09-19T08:54:37.623+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 9fbd51537104
[2024-09-19T08:54:37.784+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T08:54:37.788+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T08:54:37.822+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T08:54:37.832+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T08:56:01.738+0000] {docker.py:438} INFO - Processing batch 1/1
[2024-09-19T08:56:01.739+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240DEN.html for game date 2023-10-24
[2024-09-19T08:56:01.739+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240GSW.html for game date 2023-10-24
[2024-09-19T08:56:01.740+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250ORL.html for game date 2023-10-25
[2024-09-19T08:56:01.741+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250NYK.html for game date 2023-10-25
[2024-09-19T08:56:01.741+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250IND.html for game date 2023-10-25
[2024-09-19T08:56:01.741+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHO.html for game date 2023-10-25
[2024-09-19T08:56:01.742+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MIA.html for game date 2023-10-25
[2024-09-19T08:56:01.742+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250TOR.html for game date 2023-10-25
[2024-09-19T08:56:01.743+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250BRK.html for game date 2023-10-25
[2024-09-19T08:56:01.743+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MEM.html for game date 2023-10-25
[2024-09-19T08:56:01.743+0000] {docker.py:438} INFO - Skipping incomplete data for ja morant
[2024-09-19T08:56:01.744+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHI.html for game date 2023-10-25
[2024-09-19T08:56:01.744+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250UTA.html for game date 2023-10-25
[2024-09-19T08:56:01.745+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250SAS.html for game date 2023-10-25
[2024-09-19T08:56:01.745+0000] {docker.py:438} INFO - Skipping incomplete data for devonte' graham
[2024-09-19T08:56:01.745+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250LAC.html for game date 2023-10-25
[2024-09-19T08:56:01.746+0000] {docker.py:438} INFO - Failed to upload player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_08-56-01.csv: 'bytes' object has no attribute 'read'
[2024-09-19T08:56:01.746+0000] {docker.py:438} INFO - 2024-09-19 08:56:01,736 - ERROR - Error uploading data to MinIO: 'bytes' object has no attribute 'read'
[2024-09-19T08:56:02.018+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http+docker://localhost/v1.47/containers/create

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 372, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http+docker://localhost/v1.47/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /host_mnt/private/tmp/airflowtmp0fpjl1fj")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 381, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 446, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-09-19T08:56:02.029+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T085437, end_date=20240919T085602
[2024-09-19T08:56:02.044+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T08:56:02.045+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion (Docker container failed: {'StatusCode': 1}; 207)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http+docker://localhost/v1.47/containers/create

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 372, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http+docker://localhost/v1.47/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /host_mnt/private/tmp/airflowtmp0fpjl1fj")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 381, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 446, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-09-19T08:56:02.094+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T08:56:02.119+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T08:56:02.127+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T08:59:34.631+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T08:59:34.651+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T08:59:34.661+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T08:59:34.661+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T08:59:34.678+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T08:59:34.686+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp4g7ecfrl']
[2024-09-19T08:59:34.691+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=200) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T08:59:34.692+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T08:59:34.692+0000] {standard_task_runner.py:72} INFO - Started process 203 to run task
[2024-09-19T08:59:34.746+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host b0a2bec922a8
[2024-09-19T08:59:34.852+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T08:59:34.853+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T08:59:34.885+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T08:59:34.895+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T09:01:05.376+0000] {docker.py:438} INFO - Processing batch 1/1
[2024-09-19T09:01:05.377+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240DEN.html for game date 2023-10-24
[2024-09-19T09:01:05.377+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240GSW.html for game date 2023-10-24
[2024-09-19T09:01:05.378+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250ORL.html for game date 2023-10-25
[2024-09-19T09:01:05.378+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250NYK.html for game date 2023-10-25
[2024-09-19T09:01:05.379+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250IND.html for game date 2023-10-25
[2024-09-19T09:01:05.379+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHO.html for game date 2023-10-25
[2024-09-19T09:01:05.379+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MIA.html for game date 2023-10-25
[2024-09-19T09:01:05.380+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250TOR.html for game date 2023-10-25
[2024-09-19T09:01:05.380+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250BRK.html for game date 2023-10-25
[2024-09-19T09:01:05.381+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MEM.html for game date 2023-10-25
[2024-09-19T09:01:05.381+0000] {docker.py:438} INFO - Skipping incomplete data for ja morant
[2024-09-19T09:01:05.382+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHI.html for game date 2023-10-25
[2024-09-19T09:01:05.382+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250UTA.html for game date 2023-10-25
[2024-09-19T09:01:05.383+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250SAS.html for game date 2023-10-25
[2024-09-19T09:01:05.383+0000] {docker.py:438} INFO - Skipping incomplete data for devonte' graham
[2024-09-19T09:01:05.383+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250LAC.html for game date 2023-10-25
[2024-09-19T09:01:05.384+0000] {docker.py:438} INFO - Failed to upload player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_09-01-05.csv: 'bytes' object has no attribute 'read'
[2024-09-19T09:01:05.384+0000] {docker.py:438} INFO - 2024-09-19 09:01:05,374 - ERROR - Error uploading data to MinIO: 'bytes' object has no attribute 'read'
[2024-09-19T09:01:05.673+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http+docker://localhost/v1.47/containers/create

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 372, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http+docker://localhost/v1.47/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /host_mnt/private/tmp/airflowtmp82m5vyye")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 381, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 446, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-09-19T09:01:05.682+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T085934, end_date=20240919T090105
[2024-09-19T09:01:05.698+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T09:01:05.699+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 2 for task run_data_ingestion (Docker container failed: {'StatusCode': 1}; 203)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http+docker://localhost/v1.47/containers/create

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 372, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 399, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 440, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 457, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http+docker://localhost/v1.47/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /host_mnt/private/tmp/airflowtmp82m5vyye")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 381, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 446, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-09-19T09:01:05.719+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T09:01:05.740+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T09:01:05.742+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T09:13:24.506+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T09:13:24.528+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T09:13:24.538+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T09:13:24.538+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T09:13:24.554+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T09:13:24.563+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpjwzm7uko']
[2024-09-19T09:13:24.566+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T09:13:24.568+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=200) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T09:13:24.569+0000] {standard_task_runner.py:72} INFO - Started process 201 to run task
[2024-09-19T09:13:24.626+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 76dded199c5f
[2024-09-19T09:13:24.745+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T09:13:24.746+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T09:13:24.779+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T09:13:24.789+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T09:14:45.448+0000] {docker.py:438} INFO - Processing batch 1/1
[2024-09-19T09:14:45.449+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240DEN.html for game date 2023-10-24
[2024-09-19T09:14:45.449+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240GSW.html for game date 2023-10-24
[2024-09-19T09:14:45.450+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250ORL.html for game date 2023-10-25
[2024-09-19T09:14:45.450+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250NYK.html for game date 2023-10-25
[2024-09-19T09:14:45.451+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250IND.html for game date 2023-10-25
[2024-09-19T09:14:45.451+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHO.html for game date 2023-10-25
[2024-09-19T09:14:45.452+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MIA.html for game date 2023-10-25
[2024-09-19T09:14:45.452+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250TOR.html for game date 2023-10-25
[2024-09-19T09:14:45.453+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250BRK.html for game date 2023-10-25
[2024-09-19T09:14:45.453+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MEM.html for game date 2023-10-25
[2024-09-19T09:14:45.453+0000] {docker.py:438} INFO - Skipping incomplete data for ja morant
[2024-09-19T09:14:45.454+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHI.html for game date 2023-10-25
[2024-09-19T09:14:45.454+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250UTA.html for game date 2023-10-25
[2024-09-19T09:14:45.454+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250SAS.html for game date 2023-10-25
[2024-09-19T09:14:45.455+0000] {docker.py:438} INFO - Skipping incomplete data for devonte' graham
[2024-09-19T09:14:45.455+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250LAC.html for game date 2023-10-25
[2024-09-19T09:14:45.456+0000] {docker.py:438} INFO - File player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_09-14-45.csv successfully uploaded to bucket nba-raw-data
[2024-09-19T09:14:45.456+0000] {docker.py:438} INFO - 2024-09-19 09:14:45,446 - INFO - Data successfully uploaded to MinIO bucket 'nba-raw-data' as 'player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_09-14-45.csv'
[2024-09-19T09:14:45.457+0000] {docker.py:438} INFO - 2024-09-19 09:14:45,446 - INFO - Data ingestion completed successfully
[2024-09-19T09:14:45.757+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T09:14:45.758+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T091324, end_date=20240919T091445
[2024-09-19T09:14:45.780+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T09:14:45.806+0000] {taskinstance.py:3900} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-19T09:14:45.808+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T21:02:33.906+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T21:02:33.928+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T21:02:33.940+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T21:02:33.940+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T21:02:33.956+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T21:02:33.964+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp_0bcj9p0']
[2024-09-19T21:02:33.967+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T21:02:33.969+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=275) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T21:02:33.970+0000] {standard_task_runner.py:72} INFO - Started process 277 to run task
[2024-09-19T21:02:34.019+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 19c433e2f996
[2024-09-19T21:02:34.123+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T21:02:34.124+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T21:02:34.157+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T21:02:34.167+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T21:04:01.361+0000] {docker.py:438} INFO - Processing batch 1/1
[2024-09-19T21:04:01.362+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240DEN.html for game date 2023-10-24
[2024-09-19T21:04:01.362+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240GSW.html for game date 2023-10-24
[2024-09-19T21:04:01.363+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250ORL.html for game date 2023-10-25
[2024-09-19T21:04:01.363+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250NYK.html for game date 2023-10-25
[2024-09-19T21:04:01.364+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250IND.html for game date 2023-10-25
[2024-09-19T21:04:01.364+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHO.html for game date 2023-10-25
[2024-09-19T21:04:01.364+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MIA.html for game date 2023-10-25
[2024-09-19T21:04:01.365+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250TOR.html for game date 2023-10-25
[2024-09-19T21:04:01.365+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250BRK.html for game date 2023-10-25
[2024-09-19T21:04:01.366+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MEM.html for game date 2023-10-25
[2024-09-19T21:04:01.366+0000] {docker.py:438} INFO - Skipping incomplete data for ja morant
[2024-09-19T21:04:01.366+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHI.html for game date 2023-10-25
[2024-09-19T21:04:01.367+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250UTA.html for game date 2023-10-25
[2024-09-19T21:04:01.367+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250SAS.html for game date 2023-10-25
[2024-09-19T21:04:01.368+0000] {docker.py:438} INFO - Skipping incomplete data for devonte' graham
[2024-09-19T21:04:01.368+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250LAC.html for game date 2023-10-25
[2024-09-19T21:04:01.368+0000] {docker.py:438} INFO - File player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_21-04-01.csv successfully uploaded to bucket nba-raw-data
[2024-09-19T21:04:01.369+0000] {docker.py:438} INFO - 2024-09-19 21:04:01,359 - INFO - Data successfully uploaded to MinIO bucket 'nba-raw-data' as 'player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_21-04-01.csv'
[2024-09-19T21:04:01.369+0000] {docker.py:438} INFO - 2024-09-19 21:04:01,359 - INFO - Data ingestion completed successfully
[2024-09-19T21:04:01.662+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T21:04:01.662+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T210233, end_date=20240919T210401
[2024-09-19T21:04:01.701+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T21:04:01.723+0000] {taskinstance.py:3900} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-19T21:04:01.727+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T22:22:18.735+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T22:22:18.772+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T22:22:18.787+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T22:22:18.788+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T22:22:18.812+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_ingestion> on 2024-09-18 00:00:00+00:00
[2024-09-19T22:22:18.822+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_ingestion', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpoeuoi75p']
[2024-09-19T22:22:18.826+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask run_data_ingestion
[2024-09-19T22:22:18.829+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=210) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T22:22:18.830+0000] {standard_task_runner.py:72} INFO - Started process 213 to run task
[2024-09-19T22:22:18.891+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_ingestion scheduled__2024-09-18T00:00:00+00:00 [running]> on host 72481f19ca2f
[2024-09-19T22:22:19.008+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_ingestion' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T22:22:19.011+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T22:22:19.045+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-ingestion:latest
[2024-09-19T22:23:36.319+0000] {docker.py:438} INFO - Processing batch 1/1
[2024-09-19T22:23:36.320+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240DEN.html for game date 2023-10-24
[2024-09-19T22:23:36.320+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310240GSW.html for game date 2023-10-24
[2024-09-19T22:23:36.321+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250ORL.html for game date 2023-10-25
[2024-09-19T22:23:36.321+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250NYK.html for game date 2023-10-25
[2024-09-19T22:23:36.322+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250IND.html for game date 2023-10-25
[2024-09-19T22:23:36.322+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHO.html for game date 2023-10-25
[2024-09-19T22:23:36.323+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MIA.html for game date 2023-10-25
[2024-09-19T22:23:36.323+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250TOR.html for game date 2023-10-25
[2024-09-19T22:23:36.323+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250BRK.html for game date 2023-10-25
[2024-09-19T22:23:36.324+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250MEM.html for game date 2023-10-25
[2024-09-19T22:23:36.324+0000] {docker.py:438} INFO - Skipping incomplete data for ja morant
[2024-09-19T22:23:36.324+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250CHI.html for game date 2023-10-25
[2024-09-19T22:23:36.325+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250UTA.html for game date 2023-10-25
[2024-09-19T22:23:36.325+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250SAS.html for game date 2023-10-25
[2024-09-19T22:23:36.326+0000] {docker.py:438} INFO - Skipping incomplete data for devonte' graham
[2024-09-19T22:23:36.326+0000] {docker.py:438} INFO - Scraping box score: https://www.basketball-reference.com/boxscores/202310250LAC.html for game date 2023-10-25
[2024-09-19T22:23:36.326+0000] {docker.py:438} INFO - File player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_22-23-36.csv successfully uploaded to bucket nba-raw-data
[2024-09-19T22:23:36.327+0000] {docker.py:438} INFO - 2024-09-19 22:23:36,318 - INFO - Data successfully uploaded to MinIO bucket 'nba-raw-data' as 'player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_22-23-36.csv'
[2024-09-19T22:23:36.626+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T22:23:36.626+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_ingestion, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T222218, end_date=20240919T222336
[2024-09-19T22:23:36.674+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T22:23:36.795+0000] {taskinstance.py:3900} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-19T22:23:36.815+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
