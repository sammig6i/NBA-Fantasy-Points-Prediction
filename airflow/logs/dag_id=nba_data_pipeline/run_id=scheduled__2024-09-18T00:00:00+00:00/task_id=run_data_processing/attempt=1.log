[2024-09-19T00:42:38.401+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T00:42:38.423+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T00:42:38.434+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T00:42:38.434+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2024-09-19T00:42:38.451+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_processing> on 2024-09-18 00:00:00+00:00
[2024-09-19T00:42:38.459+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_processing', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpikwhq_vz']
[2024-09-19T00:42:38.463+0000] {standard_task_runner.py:105} INFO - Job 4: Subtask run_data_processing
[2024-09-19T00:42:38.465+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=221) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T00:42:38.466+0000] {standard_task_runner.py:72} INFO - Started process 222 to run task
[2024-09-19T00:42:38.521+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [running]> on host 60f4d263d5b1
[2024-09-19T00:42:38.629+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_processing' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T00:42:38.630+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T00:42:38.663+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-processing:latest
[2024-09-19T00:42:38.673+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T00:42:39.815+0000] {docker.py:438} INFO - 2024-09-19 00:42:39,813 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7f8fa86f62f0>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T00:42:40.219+0000] {docker.py:438} INFO - 2024-09-19 00:42:40,217 - WARNING - Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7f8fa86f6620>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T00:42:41.026+0000] {docker.py:438} INFO - 2024-09-19 00:42:41,023 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7f8fa86f6b30>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T00:42:42.634+0000] {docker.py:438} INFO - 2024-09-19 00:42:42,632 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7f8fa86f6d40>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T00:42:45.845+0000] {docker.py:438} INFO - 2024-09-19 00:42:45,842 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7f8fa86f6ef0>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T00:42:45.850+0000] {docker.py:438} INFO - 2024-09-19 00:42:45,848 - ERROR - Error in data processing: HTTPConnectionPool(host='minio', port=9000): Max retries exceeded with url: /nba-raw-data?location= (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7f8fa86f70a0>: Failed to resolve 'minio' ([Errno -2] Name or service not known)"))
[2024-09-19T00:42:46.079+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T00:42:46.079+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_processing, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T004238, end_date=20240919T004246
[2024-09-19T00:42:46.128+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T00:42:46.147+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T00:42:46.149+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T01:00:13.236+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T01:00:13.256+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T01:00:13.266+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T01:00:13.267+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2024-09-19T01:00:13.282+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_processing> on 2024-09-18 00:00:00+00:00
[2024-09-19T01:00:13.289+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_processing', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpt1akpru2']
[2024-09-19T01:00:13.293+0000] {standard_task_runner.py:105} INFO - Job 5: Subtask run_data_processing
[2024-09-19T01:00:13.294+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=219) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T01:00:13.295+0000] {standard_task_runner.py:72} INFO - Started process 220 to run task
[2024-09-19T01:00:13.348+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [running]> on host 2c1409e97385
[2024-09-19T01:00:13.444+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_processing' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T01:00:13.446+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T01:00:13.475+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-processing:latest
[2024-09-19T01:00:13.483+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T01:00:14.293+0000] {docker.py:438} INFO - 2024-09-19 01:00:14,292 - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7fb60b0ba2f0>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T01:00:14.700+0000] {docker.py:438} INFO - 2024-09-19 01:00:14,697 - WARNING - Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7fb60b0ba620>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T01:00:15.508+0000] {docker.py:438} INFO - 2024-09-19 01:00:15,505 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7fb60b0bab30>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T01:00:17.116+0000] {docker.py:438} INFO - 2024-09-19 01:00:17,114 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7fb60b0bad40>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T01:00:20.323+0000] {docker.py:438} INFO - 2024-09-19 01:00:20,321 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7fb60b0baef0>: Failed to resolve 'minio' ([Errno -2] Name or service not known)")': /nba-raw-data?location=
[2024-09-19T01:00:20.328+0000] {docker.py:438} INFO - 2024-09-19 01:00:20,326 - ERROR - Error in data processing: HTTPConnectionPool(host='minio', port=9000): Max retries exceeded with url: /nba-raw-data?location= (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x7fb60b0bb0a0>: Failed to resolve 'minio' ([Errno -2] Name or service not known)"))
[2024-09-19T01:00:20.554+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T01:00:20.554+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_processing, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T010013, end_date=20240919T010020
[2024-09-19T01:00:20.578+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T01:00:20.597+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T01:00:20.599+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T01:22:03.448+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T01:22:03.468+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T01:22:03.477+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T01:22:03.478+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2024-09-19T01:22:03.492+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_processing> on 2024-09-18 00:00:00+00:00
[2024-09-19T01:22:03.499+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_processing', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmph1yfaesw']
[2024-09-19T01:22:03.502+0000] {standard_task_runner.py:105} INFO - Job 4: Subtask run_data_processing
[2024-09-19T01:22:03.504+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=216) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T01:22:03.505+0000] {standard_task_runner.py:72} INFO - Started process 217 to run task
[2024-09-19T01:22:03.553+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [running]> on host 87bbd8b92491
[2024-09-19T01:22:03.646+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_processing' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T01:22:03.647+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T01:22:03.675+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-processing:latest
[2024-09-19T01:22:03.684+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T01:22:04.430+0000] {docker.py:438} INFO - 2024-09-19 01:22:04,429 - INFO - Starting data processing...
[2024-09-19T01:22:04.431+0000] {docker.py:438} INFO - 2024-09-19 01:22:04,429 - ERROR - Error while connecting to PostgreSQL: connection to server on socket "/var/run/postgresql/.s.PGSQL.5432" failed: No such file or directory
[2024-09-19T01:22:04.431+0000] {docker.py:438} INFO - 	Is the server running locally and accepting connections on that socket?
[2024-09-19T01:22:04.431+0000] {docker.py:438} INFO - 
[2024-09-19T01:22:04.432+0000] {docker.py:438} INFO - Database connection failed.
[2024-09-19T01:22:04.432+0000] {docker.py:438} INFO - 2024-09-19 01:22:04,429 - INFO - Successfully processed player_box_scores/2023-24/nba_player_stats_2023-24_2023-10-24_to_2023-10-25_2024-09-19_01-22-01.csv
[2024-09-19T01:22:04.686+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T01:22:04.687+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_processing, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T012203, end_date=20240919T012204
[2024-09-19T01:22:04.717+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T01:22:04.733+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T01:22:04.735+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T04:57:22.261+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T04:57:22.280+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T04:57:22.290+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T04:57:22.290+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2024-09-19T04:57:22.306+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_processing> on 2024-09-18 00:00:00+00:00
[2024-09-19T04:57:22.313+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_processing', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpvvz8_45w']
[2024-09-19T04:57:22.316+0000] {standard_task_runner.py:105} INFO - Job 4: Subtask run_data_processing
[2024-09-19T04:57:22.318+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=231) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T04:57:22.319+0000] {standard_task_runner.py:72} INFO - Started process 232 to run task
[2024-09-19T04:57:22.367+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [running]> on host e600337142de
[2024-09-19T04:57:22.455+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_processing' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T04:57:22.456+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T04:57:22.460+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T04:57:22.487+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-processing:latest
[2024-09-19T04:57:22.495+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T04:57:23.274+0000] {docker.py:438} INFO - 2024-09-19 04:57:23,273 - ERROR - An error occurred during data processing: expected string or bytes-like object
[2024-09-19T04:57:23.462+0000] {python.py:240} INFO - Done. Returned value was: 2024-09-19 04:57:23,273 - ERROR - An error occurred during data processing: expected string or bytes-like object
[2024-09-19T04:57:23.485+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T04:57:23.485+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_processing, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T045722, end_date=20240919T045723
[2024-09-19T04:57:23.528+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T04:57:23.546+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T04:57:23.549+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T05:38:10.874+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T05:38:10.896+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T05:38:10.907+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T05:38:10.908+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 2
[2024-09-19T05:38:10.922+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_processing> on 2024-09-18 00:00:00+00:00
[2024-09-19T05:38:10.931+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_processing', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp17wftary']
[2024-09-19T05:38:10.934+0000] {standard_task_runner.py:105} INFO - Job 6: Subtask run_data_processing
[2024-09-19T05:38:10.936+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=277) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T05:38:10.936+0000] {standard_task_runner.py:72} INFO - Started process 278 to run task
[2024-09-19T05:38:10.986+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [running]> on host c3e696a8f912
[2024-09-19T05:38:11.080+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_processing' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T05:38:11.081+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T05:38:11.086+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T05:38:11.116+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-processing:latest
[2024-09-19T05:38:11.125+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T05:38:12.025+0000] {docker.py:438} INFO - 2024-09-19 05:38:12,024 - ERROR - An error occurred during data processing: expected string or bytes-like object
[2024-09-19T05:38:12.204+0000] {python.py:240} INFO - Done. Returned value was: 2024-09-19 05:38:12,024 - ERROR - An error occurred during data processing: expected string or bytes-like object
[2024-09-19T05:38:12.227+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T05:38:12.228+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_processing, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T053810, end_date=20240919T053812
[2024-09-19T05:38:12.272+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T05:38:12.291+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T05:38:12.293+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T06:15:42.205+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T06:15:42.226+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T06:15:42.236+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T06:15:42.236+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T06:15:42.250+0000] {taskinstance.py:2888} INFO - Executing <Task(_PythonDecoratedOperator): run_data_processing> on 2024-09-18 00:00:00+00:00
[2024-09-19T06:15:42.258+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_processing', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpt2m3eoct']
[2024-09-19T06:15:42.261+0000] {standard_task_runner.py:105} INFO - Job 6: Subtask run_data_processing
[2024-09-19T06:15:42.263+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=277) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T06:15:42.264+0000] {standard_task_runner.py:72} INFO - Started process 278 to run task
[2024-09-19T06:15:42.316+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [running]> on host 5edf32f722bc
[2024-09-19T06:15:42.413+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_processing' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T06:15:42.414+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T06:15:42.419+0000] {baseoperator.py:405} WARNING - DockerOperator.execute cannot be called outside TaskInstance!
[2024-09-19T06:15:42.446+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-processing:latest
[2024-09-19T06:15:43.299+0000] {docker.py:438} INFO - 2024-09-19 06:15:43,297 - ERROR - An error occurred during data processing: expected string or bytes-like object
[2024-09-19T06:15:43.477+0000] {python.py:240} INFO - Done. Returned value was: 2024-09-19 06:15:43,297 - ERROR - An error occurred during data processing: expected string or bytes-like object
[2024-09-19T06:15:43.502+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T06:15:43.502+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_processing, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T061542, end_date=20240919T061543
[2024-09-19T06:15:43.551+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T06:15:43.573+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T06:15:43.575+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T09:14:46.644+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T09:14:46.676+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T09:14:46.692+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T09:14:46.693+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T09:14:46.714+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_processing> on 2024-09-18 00:00:00+00:00
[2024-09-19T09:14:46.724+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_processing', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmp9uem749x']
[2024-09-19T09:14:46.728+0000] {standard_task_runner.py:105} INFO - Job 4: Subtask run_data_processing
[2024-09-19T09:14:46.729+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=216) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T09:14:46.730+0000] {standard_task_runner.py:72} INFO - Started process 220 to run task
[2024-09-19T09:14:46.789+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [running]> on host 76dded199c5f
[2024-09-19T09:14:46.903+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_processing' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T09:14:46.905+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T09:14:46.952+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-processing:latest
[2024-09-19T09:14:46.962+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T09:14:47.960+0000] {docker.py:438} INFO - 2024-09-19 09:14:47,958 - INFO - Starting data processing...
[2024-09-19T09:14:47.963+0000] {docker.py:438} INFO - 2024-09-19 09:14:47,962 - INFO - Database connected.
[2024-09-19T09:14:47.964+0000] {docker.py:438} INFO - 2024-09-19 09:14:47,962 - INFO - Cleaning and preprocessing data...
[2024-09-19T09:14:48.001+0000] {docker.py:438} INFO - 2024-09-19 09:14:47,999 - INFO - Validation passed.
[2024-09-19T09:14:48.001+0000] {docker.py:438} INFO - 2024-09-19 09:14:47,999 - INFO - Assigning unique IDs for players and games...
[2024-09-19T09:14:48.142+0000] {docker.py:438} INFO - 2024-09-19 09:14:48,137 - INFO - Inserting player stats into database...
[2024-09-19T09:14:48.147+0000] {docker.py:438} INFO - 2024-09-19 09:14:48,145 - ERROR - Error processing raw data: duplicate key value violates unique constraint "playerstats_pkey"
[2024-09-19T09:14:48.148+0000] {docker.py:438} INFO - DETAIL:  Key (game_id, player_id)=(ffdecbe4171e4829937e63a32f0d0dad, 1) already exists.
[2024-09-19T09:14:48.148+0000] {docker.py:438} INFO - 2024-09-19 09:14:48,146 - INFO - Database connection closed.
[2024-09-19T09:14:48.149+0000] {docker.py:438} INFO - 2024-09-19 09:14:48,146 - ERROR - Data processing failed.
[2024-09-19T09:14:48.393+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T09:14:48.393+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_processing, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T091446, end_date=20240919T091448
[2024-09-19T09:14:48.464+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T09:14:48.483+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T09:14:48.485+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T21:04:02.053+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T21:04:02.075+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T21:04:02.086+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T21:04:02.086+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T21:04:02.102+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_processing> on 2024-09-18 00:00:00+00:00
[2024-09-19T21:04:02.109+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_processing', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpcb1heq7l']
[2024-09-19T21:04:02.112+0000] {standard_task_runner.py:105} INFO - Job 5: Subtask run_data_processing
[2024-09-19T21:04:02.125+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=299) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T21:04:02.126+0000] {standard_task_runner.py:72} INFO - Started process 300 to run task
[2024-09-19T21:04:02.181+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [running]> on host 19c433e2f996
[2024-09-19T21:04:02.277+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_processing' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T21:04:02.278+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T21:04:02.305+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-processing:latest
[2024-09-19T21:04:02.313+0000] {docker.py:375} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-09-19T21:04:03.069+0000] {docker.py:438} INFO - 2024-09-19 21:04:03,068 - INFO - Starting data processing...
[2024-09-19T21:04:03.073+0000] {docker.py:438} INFO - 2024-09-19 21:04:03,072 - INFO - Database connected.
[2024-09-19T21:04:03.074+0000] {docker.py:438} INFO - 2024-09-19 21:04:03,072 - INFO - Cleaning and preprocessing data...
[2024-09-19T21:04:03.106+0000] {docker.py:438} INFO - 2024-09-19 21:04:03,105 - INFO - Validation passed.
[2024-09-19T21:04:03.106+0000] {docker.py:438} INFO - 2024-09-19 21:04:03,105 - INFO - Assigning unique IDs for players and games...
[2024-09-19T21:04:03.241+0000] {docker.py:438} INFO - 2024-09-19 21:04:03,239 - INFO - Inserting player stats into database...
[2024-09-19T21:04:03.248+0000] {docker.py:438} INFO - 2024-09-19 21:04:03,247 - ERROR - Error processing raw data: duplicate key value violates unique constraint "playerstats_pkey"
[2024-09-19T21:04:03.249+0000] {docker.py:438} INFO - DETAIL:  Key (game_id, player_id)=(ffdecbe4171e4829937e63a32f0d0dad, 1) already exists.
[2024-09-19T21:04:03.249+0000] {docker.py:438} INFO - 2024-09-19 21:04:03,247 - INFO - Database connection closed.
[2024-09-19T21:04:03.249+0000] {docker.py:438} INFO - 2024-09-19 21:04:03,247 - ERROR - Data processing failed.
[2024-09-19T21:04:03.487+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T21:04:03.487+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=nba_data_pipeline, task_id=run_data_processing, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T210402, end_date=20240919T210403
[2024-09-19T21:04:03.544+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-09-19T21:04:03.580+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T21:04:03.582+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-09-19T22:23:37.491+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-19T22:23:37.512+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T22:23:37.523+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [queued]>
[2024-09-19T22:23:37.523+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 4
[2024-09-19T22:23:37.539+0000] {taskinstance.py:2888} INFO - Executing <Task(DockerOperator): run_data_processing> on 2024-09-18 00:00:00+00:00
[2024-09-19T22:23:37.547+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'nba_data_pipeline', 'run_data_processing', 'scheduled__2024-09-18T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/nba_data_pipeline.py', '--cfg-path', '/tmp/tmpv6843res']
[2024-09-19T22:23:37.550+0000] {standard_task_runner.py:105} INFO - Job 4: Subtask run_data_processing
[2024-09-19T22:23:37.551+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=226) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-09-19T22:23:37.553+0000] {standard_task_runner.py:72} INFO - Started process 227 to run task
[2024-09-19T22:23:37.603+0000] {task_command.py:467} INFO - Running <TaskInstance: nba_data_pipeline.run_data_processing scheduled__2024-09-18T00:00:00+00:00 [running]> on host 72481f19ca2f
[2024-09-19T22:23:37.704+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='nba_data_pipeline' AIRFLOW_CTX_TASK_ID='run_data_processing' AIRFLOW_CTX_EXECUTION_DATE='2024-09-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-18T00:00:00+00:00'
[2024-09-19T22:23:37.705+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-09-19T22:23:37.734+0000] {docker.py:367} INFO - Starting docker container from image us-west1-docker.pkg.dev/nba-fantasy-ml/nba-data-pipeline-repo/data-processing:latest
[2024-09-19T22:23:38.688+0000] {docker.py:438} INFO - 2024-09-19 22:23:38,686 - INFO - Starting data processing...
[2024-09-19T22:23:38.691+0000] {docker.py:438} INFO - 2024-09-19 22:23:38,690 - INFO - Database connected.
[2024-09-19T22:23:38.692+0000] {docker.py:438} INFO - 2024-09-19 22:23:38,690 - INFO - Cleaning and preprocessing data...
[2024-09-19T22:23:38.732+0000] {docker.py:438} INFO - 2024-09-19 22:23:38,731 - INFO - Validation passed.
[2024-09-19T22:23:38.733+0000] {docker.py:438} INFO - 2024-09-19 22:23:38,731 - INFO - Assigning unique IDs for players and games...
[2024-09-19T22:23:38.885+0000] {docker.py:438} INFO - 2024-09-19 22:23:38,883 - INFO - Inserting player stats into database...
[2024-09-19T22:23:38.895+0000] {docker.py:438} INFO - 2024-09-19 22:23:38,892 - ERROR - Error processing raw data: duplicate key value violates unique constraint "playerstats_pkey"
[2024-09-19T22:23:38.895+0000] {docker.py:438} INFO - DETAIL:  Key (game_id, player_id)=(ffdecbe4171e4829937e63a32f0d0dad, 1) already exists.
[2024-09-19T22:23:38.896+0000] {docker.py:438} INFO - 2024-09-19 22:23:38,892 - INFO - Database connection closed.
[2024-09-19T22:23:38.896+0000] {docker.py:438} INFO - 2024-09-19 22:23:38,892 - ERROR - Data processing failed.
[2024-09-19T22:23:39.099+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 446, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-09-19T22:23:39.108+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=nba_data_pipeline, task_id=run_data_processing, run_id=scheduled__2024-09-18T00:00:00+00:00, execution_date=20240918T000000, start_date=20240919T222337, end_date=20240919T222339
[2024-09-19T22:23:39.128+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-09-19T22:23:39.129+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 4 for task run_data_processing (Docker container failed: {'StatusCode': 1}; 227)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 513, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 384, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 446, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-09-19T22:23:39.170+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-09-19T22:23:39.189+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-19T22:23:39.191+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
